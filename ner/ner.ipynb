{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline # Very important for Production step\n",
    "from tqdm import tqdm # Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trivia10k13.csv', 'trivia10k13.bio']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"../data/trivia10k13.bio\"\n",
    "dataset_file_csv = \"../data/trivia10k13.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "dataset = []\n",
    "sentence_idx = 0\n",
    "\n",
    "if not os.path.exists(dataset_file_csv):\n",
    "    with open(dataset_file, encoding=\"utf-8\") as f:\n",
    "        flines = f.readlines()\n",
    "\n",
    "        for line in tqdm(flines, desc=\"Building dataset...\"):\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                sentence_idx += 1\n",
    "                continue\n",
    "            targ, word = line.split(\"\\t\")\n",
    "            dataset.append([\"Sentence: {}\".format(sentence_idx), word, targ])\n",
    "            \n",
    "\n",
    "    df = pd.DataFrame(dataset, columns=[\"Sentence #\", \"Word\", \"Tag\"])\n",
    "    df.to_csv(dataset_file_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples_rate = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_file_csv)\n",
    "df = df[: int(len(df) * max_samples_rate / 100)]\n",
    "df.isnull().sum()\n",
    "df = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1960, 5617, 25)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentence #'].nunique(), df.Word.nunique(), df.Tag.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 0</td>\n",
       "      <td>steve</td>\n",
       "      <td>B-Actor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 0</td>\n",
       "      <td>mcqueen</td>\n",
       "      <td>I-Actor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 0</td>\n",
       "      <td>provided</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 0</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 0</td>\n",
       "      <td>thrilling</td>\n",
       "      <td>B-Plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #       Word      Tag\n",
       "0  Sentence: 0      steve  B-Actor\n",
       "1  Sentence: 0    mcqueen  I-Actor\n",
       "2  Sentence: 0   provided        O\n",
       "3  Sentence: 0          a        O\n",
       "4  Sentence: 0  thrilling   B-Plot"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39705, 7577)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('Tag', axis=1)\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(X.to_dict('records'))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-Actor',\n",
       " 'B-Award',\n",
       " 'B-Character_Name',\n",
       " 'B-Director',\n",
       " 'B-Genre',\n",
       " 'B-Opinion',\n",
       " 'B-Origin',\n",
       " 'B-Plot',\n",
       " 'B-Quote',\n",
       " 'B-Relationship',\n",
       " 'B-Soundtrack',\n",
       " 'B-Year',\n",
       " 'I-Actor',\n",
       " 'I-Award',\n",
       " 'I-Character_Name',\n",
       " 'I-Director',\n",
       " 'I-Genre',\n",
       " 'I-Opinion',\n",
       " 'I-Origin',\n",
       " 'I-Plot',\n",
       " 'I-Quote',\n",
       " 'I-Relationship',\n",
       " 'I-Soundtrack',\n",
       " 'I-Year',\n",
       " 'O']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.Tag.values\n",
    "classes = np.unique(y)\n",
    "classes = classes.tolist()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-Actor',\n",
       " 'B-Award',\n",
       " 'B-Character_Name',\n",
       " 'B-Director',\n",
       " 'B-Genre',\n",
       " 'B-Opinion',\n",
       " 'B-Origin',\n",
       " 'B-Plot',\n",
       " 'B-Quote',\n",
       " 'B-Relationship',\n",
       " 'B-Soundtrack',\n",
       " 'B-Year',\n",
       " 'I-Actor',\n",
       " 'I-Award',\n",
       " 'I-Character_Name',\n",
       " 'I-Director',\n",
       " 'I-Genre',\n",
       " 'I-Opinion',\n",
       " 'I-Origin',\n",
       " 'I-Plot',\n",
       " 'I-Quote',\n",
       " 'I-Relationship',\n",
       " 'I-Soundtrack',\n",
       " 'I-Year']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classes = classes.copy()\n",
    "new_classes.pop()\n",
    "new_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check best model from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier Accuracy: 0.7051820193848737\n",
      "PassiveAggressiveClassifier Accuracy: 0.6982370449515378\n",
      "MultinomialNB Accuracy: 0.6681675952072045\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('SGDClassifier', SGDClassifier()),\n",
    "    ('PassiveAggressiveClassifier', PassiveAggressiveClassifier()),\n",
    "    ('MultinomialNB', MultinomialNB())\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "    clf = model\n",
    "    clf.partial_fit(X_train, y_train, classes)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"{name} Accuracy: {clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         B-Actor       0.64      0.58      0.61       383\n",
      "         B-Award       0.65      0.37      0.47        30\n",
      "B-Character_Name       0.08      0.02      0.03        56\n",
      "      B-Director       0.66      0.46      0.54       138\n",
      "         B-Genre       0.50      0.63      0.56       237\n",
      "       B-Opinion       0.25      0.28      0.26        68\n",
      "        B-Origin       0.00      0.00      0.00        54\n",
      "          B-Plot       0.09      0.02      0.03       536\n",
      "         B-Quote       0.00      0.00      0.00        12\n",
      "  B-Relationship       0.58      0.36      0.44        42\n",
      "    B-Soundtrack       0.00      0.00      0.00         9\n",
      "          B-Year       0.91      0.68      0.78       214\n",
      "         I-Actor       0.84      0.45      0.58       455\n",
      "         I-Award       0.54      0.53      0.53        57\n",
      "I-Character_Name       0.50      0.03      0.06        60\n",
      "      I-Director       0.87      0.40      0.55       129\n",
      "         I-Genre       0.81      0.12      0.20       182\n",
      "       I-Opinion       0.60      0.05      0.10        57\n",
      "        I-Origin       0.65      0.28      0.39       302\n",
      "          I-Plot       0.82      0.67      0.74      5149\n",
      "         I-Quote       0.57      0.09      0.16        44\n",
      "  I-Relationship       0.11      0.35      0.17        95\n",
      "    I-Soundtrack       0.80      0.20      0.32        20\n",
      "          I-Year       0.00      0.00      0.00        24\n",
      "\n",
      "       micro avg       0.73      0.54      0.62      8353\n",
      "       macro avg       0.48      0.27      0.31      8353\n",
      "    weighted avg       0.72      0.54      0.60      8353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/popescuandrei/anaconda3/envs/playground/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/popescuandrei/anaconda3/envs/playground/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/popescuandrei/anaconda3/envs/playground/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.partial_fit(X_train, y_train, classes)\n",
    "print(classification_report(y_pred=sgd.predict(X_test), y_true=y_test, labels=new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"embedding\", v),\n",
    "    (\"model\", sgd)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I-Plot', 'I-Plot'], dtype='<U16')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict for single word\n",
    "pipe.predict([{\"Sentence #\": \"Sentence: 1\", \"Word\": \"Avengers\"}, {\"Sentence #\": \"Sentence: 1\", \"Word\": \"Endgame\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/vectorizer.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and vectorizer\n",
    "joblib.dump(pipe, \"../models/pipe.joblib\")\n",
    "joblib.dump(v, \"../models/vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I-Plot'], dtype='<U16')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pipe and test for single word\n",
    "pipe = joblib.load(\"../models/pipe.joblib\")\n",
    "pipe.predict([{\"Sentence #\": \"Sentence: 1\", \"Word\": \"Avengers\"}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
